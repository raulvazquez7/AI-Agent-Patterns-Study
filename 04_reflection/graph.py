import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from typing import List, TypedDict

from langgraph.graph import StateGraph, END

# --- Configuration ---
load_dotenv(dotenv_path="studio/.env")

if not os.getenv("OPENAI_API_KEY"):
   raise ValueError("OPENAI_API_KEY not found. Make sure it's in 04_reflection/studio/.env")

# --- 1. Define the State for the Graph ---
class ReflectionState(TypedDict):
    task: str
    code: str
    critique: str
    iterations: int
    max_iterations: int
    message_history: List[HumanMessage]

# --- 2. Define the Nodes (Actions) for the Graph ---

llm = ChatOpenAI(model="gpt-4o", temperature=0.1)

# ** NUEVO NODO **
def initializer_node(state: ReflectionState):
    """Initializes the state, especially the message history, from the input task."""
    print("--- Initializing State ---")
    # Set default max_iterations if not provided by the user
    max_iterations = state.get("max_iterations", 3)
    return {
        "message_history": [HumanMessage(content=state["task"])],
        "iterations": 0,
        "code": "",
        "critique": "",
        "max_iterations": max_iterations
    }

def producer_node(state: ReflectionState):
    """Generates or refines code based on the current state."""
    print(f"\n--- Iteration {state['iterations']}: Producer Node ---")
    messages = list(state["message_history"])
    if state["iterations"] > 0:
        refinement_instruction = """
        Please refine the code using the critiques provided.
        Output ONLY the complete, final Python code block.
        Do not include any explanations, introductory text, or conversational filler.
        """
        messages.append(HumanMessage(content=refinement_instruction))
    
    response = llm.invoke(messages)
    
    return {
        "code": response.content, 
        "iterations": state["iterations"] + 1,
        "message_history": messages + [response]
    }

def critic_node(state: ReflectionState):
    """Critiques the code generated by the producer."""
    print("--- Critic Node ---")
    reflector_prompt = [
        SystemMessage(content="""
            You are a senior software engineer and an expert in Python.
            Your role is to perform a meticulous code review.
            Critically evaluate the provided Python code based on the original task requirements.
            Look for bugs, style issues, missing edge cases, and areas for improvement.
            If the code is perfect and meets all requirements, respond with the single phrase 'CODE_IS_PERFECT'.
            Otherwise, provide a bulleted list of your critiques.
        """),
        HumanMessage(content=f"Original Task:\n{state['task']}\n\nCode to Review:\n{state['code']}")
    ]
    
    response = llm.invoke(reflector_prompt)
    critique = response.content
    
    return {
        "critique": critique,
        "message_history": state["message_history"] + [HumanMessage(content=f"Critique of the previous code:\n{critique}")]
    }

def should_continue(state: ReflectionState):
    """Determines whether to continue the loop or end."""
    if "CODE_IS_PERFECT" in state["critique"]:
        return "end"
    if state["iterations"] >= state["max_iterations"]:
        return "end"
    return "refine"

# --- 3. Build and Compile the Graph ---
workflow = StateGraph(ReflectionState)

# ** AÑADIMOS EL NUEVO NODO **
workflow.add_node("initializer", initializer_node)
workflow.add_node("producer", producer_node)
workflow.add_node("critic", critic_node)

# ** CAMBIAMOS EL PUNTO DE ENTRADA **
workflow.set_entry_point("initializer")

# ** AÑADIMOS LA ARISTA DESDE EL INICIALIZADOR **
workflow.add_edge("initializer", "producer")

# El resto de aristas se mantienen igual
workflow.add_edge("producer", "critic")
workflow.add_conditional_edges(
    "critic",
    should_continue,
    {
        "refine": "producer",
        "end": END
    }
)
app = workflow.compile()
